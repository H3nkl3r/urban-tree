{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38199b1",
   "metadata": {},
   "source": [
    "### Deep learning inference\n",
    "\n",
    "- Detect tree objects from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1848ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urbantree.setting import Setting\n",
    "\n",
    "#SETTING = \"settings/by_dop80c_1312/deepforest_r1/setting.yaml\"\n",
    "#SETTING = \"settings/by_dop80c_1312/deepforest_r2/setting.yaml\"\n",
    "#SETTING = \"settings/by_dop80c_1312/deepforest_r3/setting.yaml\"\n",
    "#SETTING = \"settings/opendata_luftbild_dop60_1312/deepforest_r1/setting.yaml\"\n",
    "#SETTING = \"settings/opendata_luftbild_dop60_1312/deepforest_r2/setting.yaml\"\n",
    "SETTING = \"settings/opendata_luftbild_dop60_1312/deepforest_r3/setting.yaml\"\n",
    "\n",
    "setting = Setting.load_deepforest_setting(SETTING)\n",
    "setting['model_inference_config']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1465225",
   "metadata": {},
   "source": [
    "- detect tree objects from dataset and the resulting bounding boxes DataFrame are saved in pickle objects.\n",
    "- The object detection can be performed on different patch sizes and\n",
    "  no extra nms is run on aggregated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urbantree.deepforest.detection import infer_images\n",
    "\n",
    "infer_images(**setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e4502",
   "metadata": {},
   "source": [
    "- Render tree canopy raster images according to the resulting bounding boxes DataFrame saved in pickle objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b64c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urbantree.deepforest.detection import postprocess_render_images\n",
    "\n",
    "dataset_img_pattern=\"*.tiff\"\n",
    "\n",
    "postprocess_render_images(**setting,\n",
    "                          dataset_img_pattern=dataset_img_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb4f5ec",
   "metadata": {},
   "source": [
    "- calculate difference among two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urbantree.deepforest.detection import calc_diff\n",
    "from urbantree.deepforest.detection import create_bbox_shapefile\n",
    "\n",
    "# tune inference parameters to increase quality\n",
    "diff_from_inference_param = {\n",
    "  'confident_min_bbox_size': 20,\n",
    "  'confident_min_score': 0.9,\n",
    "  'morphology_factor': 1,\n",
    "  'patch': [\n",
    "    {'patch_size': 1200,\n",
    "    'patch_overlap': 0.3,\n",
    "    'iou_threshold': 0.8,\n",
    "    'score_thresh': 0.95},\n",
    "    {'patch_size': 800,\n",
    "    'patch_overlap': 0.3,\n",
    "    'iou_threshold': 0.8,\n",
    "    'score_thresh': 0.9},\n",
    "    {'patch_size': 200,\n",
    "    'patch_overlap': 0.2,\n",
    "    'iou_threshold': 0.7,\n",
    "    'score_thresh': 0.8},\n",
    "    {'patch_size': 96,\n",
    "    'patch_overlap': 0.18,\n",
    "    'iou_threshold': 0.6,\n",
    "    'score_thresh': 0.8}]}\n",
    "diff_to_inference_param = {\n",
    "  'confident_min_bbox_size': 30,\n",
    "  'confident_min_score': 0.8,\n",
    "  'morphology_factor': 3,\n",
    "  'patch': [\n",
    "    {'patch_size': 1200,\n",
    "    'patch_overlap': 0.3,\n",
    "    'iou_threshold': 0.8,\n",
    "    'score_thresh': 0.2},\n",
    "    {'patch_size': 800,\n",
    "    'patch_overlap': 0.3,\n",
    "    'iou_threshold': 0.8,\n",
    "    'score_thresh': 0.1},\n",
    "    {'patch_size': 200,\n",
    "    'patch_overlap': 0.2,\n",
    "    'iou_threshold': 0.7,\n",
    "    'score_thresh': 0.01},\n",
    "    {'patch_size': 96,\n",
    "    'patch_overlap': 0.18,\n",
    "    'iou_threshold': 0.6,\n",
    "    'score_thresh': 0.01}]}\n",
    "\n",
    "calc_diff(diff_from_setting_path=\"settings/opendata_luftbild_dop60_1312/deepforest_r3/setting.yaml\",\n",
    "          diff_to_setting_path=\"settings/by_dop80c_1312/deepforest_r2/setting.yaml\",\n",
    "          aggregate_iou_threshold=0.4,\n",
    "          diff_iou_threshold=0.08,\n",
    "          diff_cover_threshold=0.1,\n",
    "          diff_from_inference_param=diff_from_inference_param,\n",
    "          diff_to_inference_param=diff_to_inference_param,\n",
    "          output_bbox_dir='temp/diff/b',\n",
    "          output_debug_img_dir='temp/diff/debug',\n",
    "          concurrency=11) \n",
    "\n",
    "create_bbox_shapefile(src_img_dir='aerial_images_resampled/opendata_luftbild_dop60_1312',\n",
    "                    src_bbox_diff='temp/diff/b/diff',\n",
    "                    output_shp_path='temp/diff/diff.shp',\n",
    "                    size_min_threshold=200, \n",
    "                    iou_threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3da448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urbantree.setting import Setting\n",
    "from urbantree.deepforest.detection import create_bbox_shapefile\n",
    "\n",
    "setting = Setting.load_deepforest_setting(\"settings/opendata_luftbild_dop60_1312/deepforest_r3/setting.yaml\")\n",
    "create_bbox_shapefile(src_img_dir='aerial_images_resampled/opendata_luftbild_dop60_1312',\n",
    "                    src_bbox_diff='interim/opendata_luftbild_dop60_1312/deepforest_r3/inference/b',\n",
    "                    output_shp_path='temp/2017.shp',\n",
    "                    model_inference_config=setting['model_inference_config'],\n",
    "                    size_min_threshold=200,\n",
    "                    iou_threshold=0.2,\n",
    "                    geometry_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa1adc",
   "metadata": {},
   "source": [
    "# generate heatmap of tree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a90cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supermercado\n",
    "import mercantile\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def geerate_tile_def_from_feature(features, zooms, projected='mercator'):\n",
    "    \"\"\"\n",
    "        yield [x, y, z, xmin, ymin, xmax, ymax]\n",
    "        @param features\n",
    "               a list  geojson features (i.e. polygon) objects\n",
    "        @param zooms\n",
    "                a list of zoom levels\n",
    "        @param projected\n",
    "                'mercator' or 'geographic'\n",
    "    \"\"\"\n",
    "    # $ supermercado burn <zoom>\n",
    "    features = [f for f in supermercado.super_utils.filter_features(features)]\n",
    "    for zoom in zooms:\n",
    "        zr = zoom.split(\"-\")\n",
    "        for z in range(int(zr[0]),  int(zr[1] if len(zr) > 1 else zr[0]) + 1):\n",
    "            for t in supermercado.burntiles.burn(features, z):\n",
    "                tile = t.tolist()\n",
    "                # $ mercantile shapes --mercator\n",
    "                feature = mercantile.feature(\n",
    "                    tile,\n",
    "                    fid=None,\n",
    "                    props={},\n",
    "                    projected=projected,\n",
    "                    buffer=None,\n",
    "                    precision=None\n",
    "                )\n",
    "                bbox = feature[\"bbox\"]\n",
    "                yield tile + bbox\n",
    "\n",
    "TREE_DATA = 'temp/2017.shp'\n",
    "BOUNDARY = 'contrib/munich/munich.boundary.geojson'\n",
    "SCALE = 0.98\n",
    "OUTPUT = 'temp/diff/heatmap'\n",
    "\n",
    "# shrunk boundary to avoid edge cases\n",
    "b = gpd.read_file(BOUNDARY)\n",
    "b.geometry = b.geometry.scale(xfact=SCALE, yfact=SCALE)\n",
    "data = json.loads(b.geometry.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581acbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tree data\n",
    "trees = gpd.read_file(TREE_DATA).to_crs('EPSG:3857')\n",
    "print(\"{n} trees loaded\".format(n=len(trees)))\n",
    "trees.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d10c1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 54/528 [33:53<5:35:17, 42.44s/it]"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import shapely\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# TODO: make faster!\n",
    "\n",
    "NEIGHBORHOOD = 250 # meter \n",
    "SIZE = 256 # px\n",
    "ZOOM = range(15, 16)\n",
    "#COLORMAP = cm.get_cmap('RdYlGn')\n",
    "COLORMAP = cm.get_cmap('YlGn')\n",
    "\n",
    "CONTINUE = True\n",
    "\n",
    "# render each tile\n",
    "def render(x, y, z, *bbox, continue_mode=True):\n",
    "  # output png path\n",
    "  dest = (Path(OUTPUT) / str(z) / str(x)).joinpath(str(y) + \".png\")\n",
    "  if continue_mode and dest.exists():\n",
    "    return\n",
    "  bboxes = []\n",
    "  for iy in range(SIZE):\n",
    "    ymin = bbox[3] - (iy+1)*(bbox[3]-bbox[1])/SIZE\n",
    "    ymax = bbox[3] -    iy *(bbox[3]-bbox[1])/SIZE\n",
    "    if ymax-ymin < NEIGHBORHOOD:\n",
    "      ext = 0.5*(NEIGHBORHOOD-(ymax-ymin))\n",
    "      ymax = ymax + int(ext)\n",
    "      ymin = ymin - int(ext)\n",
    "    for ix in range(SIZE):\n",
    "      xmin = bbox[0] +    ix *(bbox[2]-bbox[0])/SIZE \n",
    "      xmax = bbox[0] + (ix+1)*(bbox[2]-bbox[0])/SIZE\n",
    "      if xmax-xmin < NEIGHBORHOOD:\n",
    "        ext = 0.5*(NEIGHBORHOOD-(xmax-xmin))\n",
    "        xmax = xmax + ext\n",
    "        xmin = xmin - ext\n",
    "      tile_area = (xmax-xmin)*(ymax-ymin)\n",
    "      bboxes.append([ix, iy, tile_area,\n",
    "                     shapely.geometry.box(xmin, ymin, xmax, ymax)])    \n",
    "  bboxes = pd.DataFrame(bboxes, columns=['ix','iy','tile_area','geometry'])\n",
    "  bboxes = gpd.GeoDataFrame(bboxes, geometry='geometry', crs=3857)\n",
    "  # find tree ratio for each bbox (PIXEL)\n",
    "  within = gpd.sjoin(trees, bboxes, predicate='within')\n",
    "  if len(within):\n",
    "    within['tree_ratio'] = (within.xmax-within.xmin)*(within.ymax-within.ymin)/within.tile_area\n",
    "    within = within.groupby(['iy','ix']).apply(lambda x: x.tree_ratio.sum())\n",
    "    within = within.reset_index(name='tree_ratio')\n",
    "  # draw \n",
    "  array = np.zeros([SIZE, SIZE, 4], dtype=np.uint8)\n",
    "  array[:] = (np.array(COLORMAP(0)[:4]) * 255).astype('uint8').tolist()\n",
    "  for _, row in within.iterrows():\n",
    "    color = (np.array(COLORMAP(row.tree_ratio)[:4]) * 255).astype('uint8').tolist()\n",
    "    array[int(row.iy),int(row.ix)] = color\n",
    "  im = Image.fromarray(array)\n",
    "  dest.parent.mkdir(parents=True,exist_ok=True)\n",
    "  im.save(dest)\n",
    "\n",
    "# covering tile coordinates\n",
    "tiles = geerate_tile_def_from_feature(data['features'], list(map(lambda x: str(x), ZOOM)))\n",
    "for [x, y, z, *bbox] in tqdm(list(tiles)):\n",
    "  render(x, y, z, *bbox, continue_mode=CONTINUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f68ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d17c320a108ed353692a9d869777d22c4cfc8e217512b3979b490c9a0e17a049"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
